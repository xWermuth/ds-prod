{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sparselsh import LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-based, regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load food.com data\n",
    "directory = 'data/food.com'\n",
    "df_recipe_rating = pd.read_csv(f'{directory}/recipe_ratings.csv')\n",
    "df_recipe = pd.read_csv(f'{directory}/recipe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipe_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXERCISE: Build a content-based recommender system that uses linear regression \n",
    "#          to predict ratings.\n",
    "#          Try it out on users with a high number of ratings.\n",
    "#          Try some train-test split to evaluate performance.\n",
    "\n",
    "#select the ratings of a specific user \n",
    "target_id = ...\n",
    "df_user = df_recipe_rating[df_recipe_rating['user_id']==target_id][['recipe_id','rating']]\n",
    "df_rec = pd.merge(df_user, df_recipe, on='recipe_id', how='inner')\n",
    "\n",
    "# define features\n",
    "features = [...] \n",
    "\n",
    "# split training and test\n",
    "...\n",
    "\n",
    "# fit on the training, test on the rest\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-based, KNN (with LSH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data/movielens/ml-latest-small'\n",
    "#directory = 'data/movielens/ml-latest' #change into this one for the full dataset (slow)\n",
    "\n",
    "df_movies = pd.read_csv(f'{directory}/movies.csv')\n",
    "df_ratings = pd.read_csv(f'{directory}/ratings.csv')\n",
    "df_tags = pd.read_csv(f'{directory}/tags.csv')\n",
    "\n",
    "#transform tags such that they are lower-case, single-word tokens\n",
    "df_tags['tag'] = df_tags['tag'].apply(lambda x: str(x).lower().replace(' ', '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Calculate item profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the lexicon of most frequent tags.\n",
    "tag_frequency_threshold = 5 # increase number to filter\n",
    "df_lexicon = ... # get a dataframe with tags and respective counts\n",
    "\n",
    "# discard movies with no tags\n",
    "...\n",
    "\n",
    "# you can drop the userId and timestamp columns because we don't care who assigned the tag and when\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the sparse feature vector based on the TF-IDF of words in documents\n",
    "#the TF-IDF vectors are saved as sparse representations into the dataframe\n",
    "df_features = df_tags.groupby('movieId').agg(lambda x: ' '.join(x)).reset_index()\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(' ')).fit(sorted(df_features['tag']))\n",
    "vectorizer.vocabulary_\n",
    "df_features['feature_vector'] = df_features['tag'].apply(lambda x : vectorizer.transform([x]))\n",
    "df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Index item profiles into LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index all item vectors into LSH\n",
    "lsh = LSH(...)\n",
    "\n",
    "#run an example query to the LSH\n",
    "lsh.query(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate user profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restricts the ratings to the set of most popular movies (optional, not needed for content-based)\n",
    "numratings_threshold = 0 #increase this number if you want to filter\n",
    "df_item_popularity = df_ratings[['movieId','rating']].groupby('movieId').count().reset_index()\n",
    "df_item_popularity.columns = ['movieId','count'] \n",
    "df_item_popularity = df_item_popularity.sort_values(by='count', ascending=False)\n",
    "df_item_popularity = df_item_popularity[df_item_popularity['count'] >= numratings_threshold]\n",
    "print(f'Number of movies reduced from {len(df_ratings.movieId.unique())} to {len(df_item_popularity.movieId.unique())}')\n",
    "df_ratings = pd.merge(df_ratings, df_item_popularity, on='movieId', how='inner')[['userId', 'movieId', 'rating']]\n",
    "df_ratings = df_ratings.sort_values(by='userId')\n",
    "\n",
    "#rescale the ratings by the user's individual average \n",
    "df_ratings['rating_scaled'] = ...\n",
    "\n",
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join ratings with movie feature vectors\n",
    "df_profile = pd.merge(df_ratings, df_features[['movieId','feature_vector']],\n",
    "              on='movieId')\n",
    "#scaling feature vector by rating (this will take a few minutes)\n",
    "df_profile['feature_vector_scaled'] = df_profile['rating_scaled'] * df_profile['feature_vector']\n",
    "df_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#stack all sparse vectors of user's movies\n",
    "df_user_vectors = df_profile[['userId', 'feature_vector_scaled']].groupby('userId').agg(sparse.vstack).reset_index()\n",
    "#compute the average of the vectors without considering the zero entries (this will take a while)\n",
    "df_user_vectors['feature_vector_scaled'] = df_user_vectors['feature_vector_scaled'].apply(lambda x: csr_matrix(np.nan_to_num(x.sum(axis=0)/x.getnnz(axis=0), 0)))\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "df_user_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Rank potential recommendation candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick a target user to provide recommendations to\n",
    "idx = 42\n",
    "target_userId = df_user_vectors.iloc[idx].userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get user rating history\n",
    "df_user_history = ...\n",
    "\n",
    "#select candidate recommendations to user\n",
    "df_recommendation = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_history.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_history.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Predict ratings of candidate items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index all user vectors into LSH\n",
    "df_usr = df_profile[df_profile['userId'] == target_userId]\n",
    "lsh_usr = LSH(...)\n",
    "lsh_usr.index(..., extra_data=[...]) # repeat for all users. Insert movieid and rating as extra data for future retrieval\n",
    "lsh_usr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute recommendation\n",
    "df_recommendation = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.prediction_algorithms.knns import KNNBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data/movielens/ml-latest-small'\n",
    "#directory = 'data/movielens/ml-latest' #change into this one for the full dataset (slow)\n",
    "df_ratings = pd.read_csv(f'{directory}/ratings.csv')\n",
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a data reader\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "# provide a dataset with userid, itemtid, and rating in order\n",
    "data = Dataset.load_from_df(df_ratings[['userId','movieId','rating']], reader)\n",
    "\n",
    "# surprise has also some built-in datasets that can be imported directly\n",
    "#data = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a user-based K nearest neighbors implementation\n",
    "...\n",
    "# execute 5-fold cross-validation and measure RMSE and MAE\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
